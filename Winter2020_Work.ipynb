{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Winter2020_Work",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN9D9rK2Cny6m/rqV0YiihY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmadduri/BMI_Model/blob/brainAdaptOnly/Winter2020_Work.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gNWDsMuz1Ok",
        "colab_type": "text"
      },
      "source": [
        "**Start with this:**\n",
        "\n",
        "Run the (***simple random search***) iteration\n",
        "\n",
        "$$ u^+ = u - \\frac{\\gamma}{N} \\sum_{n = 1}^N ( c(u + u_n) - c(u) ) \\cdot u_n,\\ u_n \\sim \\mathcal{N}(u,\\sigma^2) $$\n",
        "\n",
        "In our case, the cost function is just the reach error -- calculated by the reach error between the previous lamdba values and the current one. \n",
        "\n",
        "\n",
        "Error is defined as reach error: $ error = ||t-y||^2 $ and the perturbation term $ p_{2k+1} $ can be thought of a normal distribution.\n",
        "\n",
        "$$ \\lambda^+ = \\lambda - \\frac{\\nu}{N\\delta} \\sum_{n = 1}^N ( error(\\lambda + \\delta p) - error(\\lambda) ) \\cdot  p, p \\sim \\mathcal{N}(\\lambda,\\sigma^2) $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUGsMMygdWvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Function that generates the adaptation\n",
        "\n",
        "\n",
        "\n",
        "####################\n",
        "# Brain Model \n",
        "def calcNextLambda(lambda_vect, gamma, delta_perturb, targ_vect):\n",
        "  # This is the vector to multiply the lambda update term with = [1 t_x t_y]'\n",
        "  targ_vect_mult = np.insert(targ_vect, 0, 1)\n",
        "  return lambda_vect - (gamma*delta_perturb*targ_vect_mult)\n",
        "\n",
        "def brainFiringRate(lambda_vect, targ_vect):\n",
        "  # f = b + Fwt \n",
        "  # [b w_x w_y]*[1 t_x  t_y ]'\n",
        "  targ_vect_mult = np.insert(targ_vect, 0, 1)\n",
        "  newFR = np.matmul(lambda_vect, targ_vect_mult) \n",
        "  return newFR\n",
        "\n",
        "####################\n",
        "# Decoder Model \n",
        "# K_matx = num neurons x num target \n",
        "K_matx = np.array([[0.5, 0.2]])\n",
        "\n",
        "# keyword arguments--like a static variable (add states to function)\n",
        "# make it's optional argument (kwargs)\n",
        "# might have to make neurons_num global\n",
        "# firing rate --> a + Kf --> y\n",
        "def decoder_findY(firing_rate, targ_vect):\n",
        "  global K_matx\n",
        "  # D(f) = a + Kf = y\n",
        "  # Start with affine decoder\n",
        "  a_vect = np.ones(np.size(targ_vect))*0.5\n",
        "\n",
        "  nextPred_vect = a_vect + np.dot(K_matx, np.squeeze(np.asarray(firing_rate)))\n",
        "  return (nextPred_vect)\n",
        "\n",
        "# Note to self: this function is returning a + Kf = y accurately\n",
        "# ######\n",
        "# define reach error\n",
        "# y_x, y_y = matrix of predicted cursor position\n",
        "# t_x, t_y = matrix of target position\n",
        "def calcReachError(y_vect, t_vect):\n",
        "  norm_vect = np.array(y_vect) - np.array(t_vect)\n",
        "  return (np.linalg.norm(norm_vect, 2)**2)\n",
        "\n",
        "def reachError_FR(firing_rate, targ_vect):\n",
        "  y_vect = decoder_findY(firing_rate, targ_vect)\n",
        "  t_vect = targ_vect\n",
        "  return calcReachError(y_vect, t_vect)  \n",
        "\n",
        "# find next g\n",
        "# define random search\n",
        "\n",
        "# this is only 1 iteration of SGD\n",
        "# takes in a vector lambda \n",
        "# start with scalar case\n",
        "def findErrorGrad(fr_input, sigma, delta, targ_vect):\n",
        "  N_sum = 0\n",
        "  N_neurons = np.size(fr_input)\n",
        "  N_samp = 100\n",
        "\n",
        "  perturb_rand = np.random.normal(fr_input, sigma, N_samp)\n",
        "\n",
        "  for iN in range(N_samp):\n",
        "    N_sum = N_sum + (reachError_FR(fr_input + delta*perturb_rand[iN], targ_vect) - reachError_FR(fr_input, targ_vect))*perturb_rand[iN]\n",
        "\n",
        "  errorGrad = N_sum/(N_samp*delta)\n",
        "  # print(avgUpdate)\n",
        "  return errorGrad\n",
        "\n",
        "def findErrorGrad_lambda(lambda_input, sigma, delta, targ_vect):\n",
        "  N_sum = 0\n",
        "  N_samp = 1\n",
        "\n",
        "  print(\"lambda_input\" + str(lambda_input))\n",
        "\n",
        "  fr_curr = brainFiringRate(lambda_input, targ_vect)\n",
        "\n",
        "  perturb_rand = np.zeros((np.size(lambda_input), N_samp))\n",
        "\n",
        "  for iL in range(np.size(lambda_input)):\n",
        "    perturb_rand[iL, :] = np.random.normal(lambda_input[iL], sigma, N_samp) \n",
        "    for iN in range(N_samp):\n",
        "      # generate new firing rate\n",
        "      lambda_perturb = np.add(lambda_input, delta*perturb_rand[:, iN])\n",
        "      print(\"lambda perturb = \" + str(lambda_perturb))\n",
        "      print(\"lambda_input\" + str(lambda_input))\n",
        "      print(\"perturb_rand\" + str(perturb_rand))\n",
        "      fr_perturb = brainFiringRate(lambda_perturb, targ_vect)\n",
        "      N_sum = N_sum + (reachError_FR(fr_perturb, targ_vect) - reachError_FR(fr_curr, targ_vect))*perturb_rand[iN]\n",
        "\n",
        "    avgUpdate = N_sum/(N_samp*delta)\n",
        "  # print(avgUpdate)\n",
        "  return avgUpdate\n",
        "\n",
        "# param vect = what to update \n",
        "def gradDesc(fr_init, lambda_init, targ_vect, learn_rate, numIter, sigma, delta):\n",
        "  fr_vect = np.array(fr_init)\n",
        "  lambda_vect = np.zeros((np.size(lambda_init), numIter))\n",
        "  lambda_vect[:,0] = lambda_init\n",
        "\n",
        "  err_next = reachError_FR(fr_vect[-1], targ_vect)\n",
        "  err_vect = np.array(err_next)\n",
        "\n",
        "  delta_vect = np.array(err_next)\n",
        "  \n",
        "  for iT in range(1, numIter, 1):\n",
        "    # delta_perturb = \\Delta_feedback term due to perturbation\n",
        "    # delta_perturb = findErrorGrad(fr_vect[-1], sigma, delta, targ_vect)\n",
        "\n",
        "    # fr_fb_next = fr_vect[-1] + learn_rate*delta_perturb\n",
        "    delta_perturb = findErrorGrad_lambda(lambda_vect[:, iT-1], sigma, delta, targ_vect)\n",
        "    print(\"delta perturb = \" + str(delta_perturb))\n",
        "\n",
        "    \n",
        "    # lambda+ = lambda - learn_rate*delta_perturb\n",
        "    lambda_next = calcNextLambda(lambda_vect[:, iT-1], learn_rate, delta_perturb, targ_vect)\n",
        "    print(\"next lambda = \" + str(lambda_next))\n",
        "\n",
        "    # # calculate new firing rate with lamdba+\n",
        "    fr_next = np.abs(brainFiringRate(lambda_next, targ_vect))\n",
        "\n",
        "    # calculate new reach error with lamdba+ and fr+\n",
        "    err_next = np.array(reachError_FR(fr_next, targ_vect))  \n",
        "\n",
        "    # update vectors \n",
        "    fr_vect = np.vstack( (fr_vect, fr_next) )\n",
        "    err_vect = np.vstack( (err_vect, err_next) )  \n",
        "    delta_vect = np.vstack( (delta_vect, delta_perturb) )\n",
        "    # lambda_vect = np.vstack( (lambda_vect, lambda_next) )\n",
        "    lambda_vect[:, iT] = lambda_next\n",
        "\n",
        "\n",
        "  it_idx = np.linspace(0, numIter, numIter, endpoint=False) \n",
        "  plt.figure()\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  ax.plot(it_idx, err_vect, label = 'RE')\n",
        "  ax.plot(it_idx, fr_vect, label = 'FR') \n",
        "  # ax.plot(it_idx, lambda_vect, label = 'lambda')\n",
        "  leg = ax.legend();\n",
        "  plt.grid()\n",
        "  plt.title('stoch update')\n",
        "\n",
        "  print(\"Final Reach Error = \" + str(err_vect[numIter-1]) )\n",
        "\n",
        "\n",
        "#### Code\n",
        "\n",
        "##### TESTING ONLY\n",
        "# define c, D1_c, D2_c\n",
        "def cost(u):\n",
        "  return ((u**6)/6) - (7/5 * u**5) + (17/4 * u**4) - (17/3 * u**3) + (3 * u**2)\n",
        "def D1_c(u):\n",
        "  return ((u**5) - (7 * u**4) + (17 * u**3) - (17 * u**2) + (6 * u**1))\n",
        "def D2_c(u):\n",
        "  return ((5 * u**4) - (7*4 * u** 3) + (17*3 * u**2) - (17*2 * u) + (6))\n",
        "\n",
        "#define gradient descent \n",
        "def gradDesc_cont(rate, it_num, u_init):\n",
        "  u_new = [u_init]\n",
        "  c_new = [cost(u_new[-1])]\n",
        "  for it in range(1,it_num,1):\n",
        "    u_new.append(u_new[-1] - rate*D1_c(u_new[-1]))\n",
        "    c_new.append(cost(u_new[-1]))\n",
        "  it_idx = np.linspace(0,it_num, it_num, endpoint=False) \n",
        "  plt.figure()\n",
        "  plt.plot(it_idx, c_new)\n",
        "  plt.plot(it_idx, u_new)\n",
        "\n",
        "# # Set initial conditions\n",
        "\n",
        "# #######\n",
        "# Set initial conditions\n",
        "learn_rate = 0.5\n",
        "numIter = 4\n",
        "fr_init = [0.5]\n",
        "lambda_init = np.array([0.1, 0.4, 3])\n",
        "sigma = 1e-1\n",
        "delta = 1e-3\n",
        "\n",
        "TARGET_NUM = 2\n",
        "TARGET_VECTOR = [2, 2]\n",
        "gradDesc(fr_init, lambda_init, TARGET_VECTOR, learn_rate, numIter, sigma, delta)\n",
        "\n",
        "# rate = 0.1\n",
        "# u_init = 0.5\n",
        "# gradDesc_cont(rate, numIter, u_init)\n",
        "# plt.legend({'c', 'u'})\n",
        "# plt.grid()\n",
        "\n",
        "# e1 = decoder_findY(u_init, TARGET_VECTOR)\n",
        "# print(e1)\n",
        "# print(reachError_FR(u_init, TARGET_VECTOR))\n",
        "# print(np.linalg.norm(e1, 2)**2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLJIMvmjztBi",
        "colab_type": "code",
        "outputId": "48b153cd-2a54-4de0-9384-e13bcd4adf00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Function that generates the adaptation\n",
        "\n",
        "\n",
        "\n",
        "####################\n",
        "# Brain Model \n",
        "def calcNextLambda(lambda_vect, gamma, delta_perturb, targ_vect):\n",
        "  # This is the vector to multiply the lambda update term with = [1 t_x t_y]'\n",
        "  targ_vect_mult = np.insert(targ_vect, 0, 1)\n",
        "  return lambda_vect - (gamma*delta_perturb*targ_vect_mult)\n",
        "\n",
        "## lambda, t --> f\n",
        "def brainFiringRate(lambda_vect, targ_vect):\n",
        "  # f = b + Fwt \n",
        "  # [b w_x w_y]*[1 t_x  t_y ]'\n",
        "  targ_vect_mult = np.insert(targ_vect.copy(), 0, 1)\n",
        "  newFR = np.matmul(lambda_vect, targ_vect_mult) \n",
        "  return newFR\n",
        "\n",
        "####################\n",
        "# Decoder Model \n",
        "# K_matx = num neurons x num target \n",
        "K_matx = np.array([[0.5, 0.2]])\n",
        "\n",
        "# keyword arguments--like a static variable (add states to function)\n",
        "# make it's optional argument (kwargs)\n",
        "# might have to make neurons_num global\n",
        "# firing rate --> a + Kf --> y\n",
        "## f --> y\n",
        "def decoder_findY(firing_rate, targ_vect):\n",
        "  global K_matx\n",
        "  # D(f) = a + Kf = y\n",
        "  # Start with affine decoder\n",
        "  a_vect = np.ones(np.size(targ_vect))*0.5\n",
        "\n",
        "  nextPred_vect = a_vect + np.dot(K_matx, np.squeeze(np.asarray(firing_rate)))\n",
        "  return (nextPred_vect)\n",
        "\n",
        "# Note to self: this function is returning a + Kf = y accurately\n",
        "# ######\n",
        "# define reach error\n",
        "# y_x, y_y = matrix of predicted cursor position\n",
        "# t_x, t_y = matrix of target position\n",
        "def calcReachError(y_vect, t_vect):\n",
        "  norm_vect = np.array(y_vect) - np.array(t_vect)\n",
        "  return (np.linalg.norm(norm_vect, 2)**2)\n",
        "\n",
        "def reachError_FR(firing_rate, targ_vect):\n",
        "  y_vect = decoder_findY(firing_rate, targ_vect)\n",
        "  t_vect = targ_vect\n",
        "  return calcReachError(y_vect, t_vect)  \n",
        "\n",
        "# find next g\n",
        "# define random search\n",
        "\n",
        "# this is only 1 iteration of SGD\n",
        "# takes in a vector lambda \n",
        "# start with scalar case\n",
        "def findErrorGrad(fr_input, sigma, delta, targ_vect):\n",
        "  N_sum = 0\n",
        "  N_neurons = np.size(fr_input)\n",
        "  N_samp = 100\n",
        "\n",
        "  perturb_rand = np.random.normal(fr_input, sigma, N_samp)\n",
        "\n",
        "  for iN in range(N_samp):\n",
        "    N_sum = N_sum + (reachError_FR(fr_input + delta*perturb_rand[iN], targ_vect) - reachError_FR(fr_input, targ_vect))*perturb_rand[iN]\n",
        "\n",
        "  errorGrad = N_sum/(N_samp*delta)\n",
        "  # print(avgUpdate)\n",
        "  return errorGrad\n",
        "\n",
        "def findErrorGrad_lambda(lambda_input, sigma, delta, targ_vect):\n",
        "  N_samp = 100\n",
        "  lambda_size = np.size(lambda_input)\n",
        "  errorGrad = np.zeros(lambda_size)\n",
        "  lambda_perturb = np.zeros(lambda_size)\n",
        "\n",
        "\n",
        "  # fr_curr = brainFiringRate(lambda_input, targ_vect)\n",
        "\n",
        "  perturb_rand = np.zeros(( lambda_size, N_samp ))\n",
        "  for iL in range(lambda_size): \n",
        "    perturb_rand[iL, :] = np.random.normal(lambda_input[iL], sigma, N_samp) \n",
        "    N_sum = 0\n",
        "    for iN in range(N_samp):\n",
        "      # lambda_curr = lambda_input[:]\n",
        "      lambda_perturb = lambda_input.copy()\n",
        "      # perturb_term = lambda_perturb[iL] + delta*perturb_rand[iL, iN]\n",
        "      # lambda_perturb[iL] = lambda_perturb[iL] + delta*perturb_rand[iL, iN]\n",
        "      lambda_perturb[iL] = delta*perturb_rand[iL, iN]\n",
        "\n",
        "    \n",
        "\n",
        "      error_perturb = calcReachError(decoder_findY(brainFiringRate(lambda_perturb, targ_vect), targ_vect), targ_vect)\n",
        "      error_input = calcReachError(decoder_findY(brainFiringRate(lambda_input, targ_vect), targ_vect), targ_vect)\n",
        "\n",
        "      # print everything\n",
        "      # print( \"lambda_curr = \" + str(lambda_input) )\n",
        "      # print( \"lambda_perturb = \" + str(lambda_perturb) )\n",
        "      # print(\"b_FR perturb = \" + str(brainFiringRate(lambda_perturb, targ_vect) ))\n",
        "      # print(\"find Y perturb = \" + str(decoder_findY(brainFiringRate(lambda_perturb, targ_vect), targ_vect)) )\n",
        "      # print(\"find error perturb = \" + str(error_perturb) )\n",
        "\n",
        "      # print(\"b_FR input = \" + str(brainFiringRate(lambda_input, targ_vect) ))\n",
        "      # print(\"find Y input = \" + str(decoder_findY(brainFiringRate(lambda_input, targ_vect), targ_vect)))\n",
        "      # print(\"find error input = \" + str(error_input))\n",
        "\n",
        "      # print(\"error grad = \" + str( (error_perturb - error_input)*perturb_rand[iL, iN]))\n",
        "      N_sum = N_sum + (np.abs( calcReachError(decoder_findY(brainFiringRate(lambda_perturb, targ_vect), targ_vect), targ_vect ) \n",
        "                              - calcReachError(decoder_findY(brainFiringRate(lambda_input, targ_vect), targ_vect), targ_vect)))*perturb_rand[iL, iN]  \n",
        "      # print(\"running sum = \" + str(N_sum))\n",
        "    errorGrad[iL] = N_sum/(N_samp*delta)\n",
        "  \n",
        "  # print(np.shape(errorGrad))\n",
        "  return errorGrad\n",
        "\n",
        "# param vect = what to update \n",
        "def gradDesc(fr_init, lambda_init, targ_vect, learn_rate, numIter, sigma, delta):\n",
        "  fr_vect = np.array(fr_init)\n",
        "  lambda_vect = np.zeros((np.size(lambda_init), numIter))\n",
        "  lambda_vect[:,0] = lambda_init\n",
        "\n",
        "  err_next = reachError_FR(fr_vect[-1], targ_vect)\n",
        "  err_vect = np.array(err_next)\n",
        "\n",
        "  delta_vect = np.array(err_next)\n",
        "  \n",
        "  for iT in range(1, numIter, 1):\n",
        "    # delta_perturb = \\Delta_feedback term due to perturbation\n",
        "    # delta_perturb = findErrorGrad(fr_vect[-1], sigma, delta, targ_vect)\n",
        "\n",
        "    # fr_fb_next = fr_vect[-1] + learn_rate*delta_perturb\n",
        "    delta_perturb = findErrorGrad_lambda(lambda_vect[:, iT-1], sigma, delta, targ_vect)\n",
        "    print(\"delta perturb = \" + str(delta_perturb))\n",
        "\n",
        "    \n",
        "    # lambda+ = lambda - learn_rate*delta_perturb\n",
        "    lambda_next = calcNextLambda(lambda_vect[:, iT-1], learn_rate, delta_perturb, targ_vect)\n",
        "    print(\"next lambda = \" + str(lambda_next))\n",
        "\n",
        "    # # calculate new firing rate with lamdba+\n",
        "    fr_next = np.abs(brainFiringRate(lambda_next, targ_vect))\n",
        "\n",
        "    # calculate new reach error with lamdba+ and fr+\n",
        "    err_next = np.array(reachError_FR(fr_next, targ_vect))  \n",
        "\n",
        "    # update vectors \n",
        "    fr_vect = np.vstack( (fr_vect, fr_next) )\n",
        "    err_vect = np.vstack( (err_vect, err_next) )  \n",
        "    delta_vect = np.vstack( (delta_vect, delta_perturb) )\n",
        "    # lambda_vect = np.vstack( (lambda_vect, lambda_next) )\n",
        "    lambda_vect[:, iT] = lambda_next\n",
        "\n",
        "\n",
        "  it_idx = np.linspace(0, numIter, numIter, endpoint=False) \n",
        "  plt.figure()\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  ax.plot(it_idx, err_vect, label = 'RE')\n",
        "  ax.plot(it_idx, fr_vect, label = 'FR') \n",
        "  # ax.plot(it_idx, lambda_vect, label = 'lambda')\n",
        "  leg = ax.legend();\n",
        "  plt.grid()\n",
        "  plt.title('stoch update')\n",
        "\n",
        "  print(\"Final Reach Error = \" + str(err_vect[numIter-1]) )\n",
        "\n",
        "\n",
        "#### Code\n",
        "\n",
        "##### TESTING ONLY\n",
        "# define c, D1_c, D2_c\n",
        "def cost(u):\n",
        "  return ((u**6)/6) - (7/5 * u**5) + (17/4 * u**4) - (17/3 * u**3) + (3 * u**2)\n",
        "def D1_c(u):\n",
        "  return ((u**5) - (7 * u**4) + (17 * u**3) - (17 * u**2) + (6 * u**1))\n",
        "def D2_c(u):\n",
        "  return ((5 * u**4) - (7*4 * u** 3) + (17*3 * u**2) - (17*2 * u) + (6))\n",
        "\n",
        "#define gradient descent \n",
        "def gradDesc_cont(rate, it_num, u_init):\n",
        "  u_new = [u_init]\n",
        "  c_new = [cost(u_new[-1])]\n",
        "  for it in range(1,it_num,1):\n",
        "    u_new.append(u_new[-1] - rate*D1_c(u_new[-1]))\n",
        "    c_new.append(cost(u_new[-1]))\n",
        "  it_idx = np.linspace(0,it_num, it_num, endpoint=False) \n",
        "  plt.figure()\n",
        "  plt.plot(it_idx, c_new)\n",
        "  plt.plot(it_idx, u_new)\n",
        "\n",
        "# # Set initial conditions\n",
        "\n",
        "# #######\n",
        "# Set initial conditions\n",
        "learn_rate = 0.5\n",
        "numIter = 4\n",
        "fr_init = [0.5]\n",
        "lambda_init = np.array([0.1, 0.4, 3])\n",
        "sigma = 1e-1\n",
        "delta = 1\n",
        "\n",
        "TARGET_NUM = 2\n",
        "TARGET_VECTOR = [2, 2]\n",
        "# gradDesc(fr_init, lambda_init, TARGET_VECTOR, learn_rate, numIter, sigma, delta)\n",
        "\n",
        "\n",
        "##\n",
        "\n",
        "print(findErrorGrad_lambda(lambda_init, sigma, delta, TARGET_VECTOR))\n",
        "\n",
        "##### TESTING ONLY\n",
        "# rate = 0.1\n",
        "# u_init = 0.5\n",
        "# gradDesc_cont(rate, numIter, u_init)\n",
        "# plt.legend({'c', 'u'})\n",
        "# plt.grid()\n",
        "\n",
        "# e1 = decoder_findY(u_init, TARGET_VECTOR)\n",
        "# print(e1)\n",
        "# print(reachError_FR(u_init, TARGET_VECTOR))\n",
        "# print(np.linalg.norm(e1, 2)**2)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.01655788 0.13316619 0.85032117]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}