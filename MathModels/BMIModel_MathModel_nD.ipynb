{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction:**\n",
    "\n",
    "Using simple random search. In our case, the cost function is just the reach error -- calculated by the reach error between the previous lamdba values and the current one. \n",
    "\n",
    "Error is defined as reach error: $ error = ||t-y||^2 $ and the perturbation term $ p_{2k+1} $ can be thought of a uniform distribution.\n",
    "\n",
    "$$ \\lambda^+ = \\lambda - (\\frac{\\nu}{N\\delta} \\sum_{n = 1}^N ( error(FR + \\delta p) - error(FR) ) \\cdot  p)*(dB/dW), p \\sim \\mathcal{N}(\\mu = 0, \\sigma^2) $$\n",
    "\n",
    "Note: p is a perturbation taken from a distribution with mean = 0\n",
    "\n",
    "This is based off of a ***simple random search*** algorithm\n",
    "\n",
    "$$ u^+ = u - \\frac{\\gamma}{N \\Delta} \\sum_{n = 1}^N ( c(u + \\Delta u_n) - c(u) ) \\cdot u_n,\\ u_n \\sim \\mathcal{N}(u,\\sigma^2) $$\n",
    "\n",
    "(See [3] for random search/stochastic gradient descent reference)\n",
    "\n",
    "References:\n",
    "\n",
    "[1] G. Cauwenberghs, “A Fast Stochastic Error-Descent Algorithm for Supervised Learning and Optimization,” in Advances in Neural Information Processing Systems 5, S. J. Hanson, J. D. Cowan, and C. L. Giles, Eds. Morgan-Kaufmann, 1993, pp. 244–251.\n",
    "\n",
    "[2] R. Héliot, K. Ganguly, J. Jimenez, and J. M. Carmena, “Learning in Closed-Loop Brain–Machine Interfaces: Modeling and Experimental Validation,” IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 40, no. 5, pp. 1387–1397, Oct. 2010, doi: 10.1109/TSMCB.2009.2036931.\n",
    "\n",
    "[3] Y. Nesterov and V. Spokoiny, “Random Gradient-Free Minimization of Convex Functions,” Found Comput Math, vol. 17, no. 2, pp. 527–566, Apr. 2017, doi: 10.1007/s10208-015-9296-2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test Cases:__\n",
    "\n",
    "1. Try ideal scenario: KW = Identity matrix, A = 0, B = 0\n",
    "2. Limit number of neurons to 2\n",
    "3. Test SGD against simple cost function ($y = x^2$)\n",
    "4. Ideal scenario with A/B: KW = I, A - Kb = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from numpy import linalg, matlib\n",
    "# save, import and load data\n",
    "from numpy import asarray, save, load\n",
    "# needed for some matrix manipulations\n",
    "import matplotlib.pyplot as plt\n",
    "# for plots\n",
    "import seaborn\n",
    "# to get date for saving data\n",
    "from datetime import date    \n",
    "\n",
    "\n",
    "# set up seaborn for the plots\n",
    "seaborn.set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parent_dir = os.getcwd()\n",
    "parent_dir = '/Users/mmadduri/Documents/PhD/ResearchMaterials/Code/BMI_Model'\n",
    "# # define the name of the directory to be created\n",
    "today = date.today().isoformat()\n",
    "today_path = parent_dir + \"/data/\" + today\n",
    "print(today_path)\n",
    "\n",
    "\n",
    "try:\n",
    "    # Recursive directory creation function. \n",
    "    # Like mkdir(), but makes all intermediate-level directories needed to contain the leaf directory.\n",
    "    os.makedirs(today_path)\n",
    "except OSError:\n",
    "    print (\"Directory %s already exists\" % today_path)\n",
    "else:\n",
    "    print (\"Successfully created the directory %s \" % today_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory \n",
    "os.chdir(today_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pylab\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "pylab.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brain Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brain Model \n",
    "###################\n",
    "# Function that calculates firing rate from b, W, t\n",
    "# B(t) = b + [Wx Wy][tx ty]' = firing rate\n",
    "# This is Equation (2.a, 2.b) in Heliot et al, 2010\n",
    "\n",
    "## INPUT\n",
    "# N = number of neurons\n",
    "# lambda_vect = [b, W_x, W_y] for each neuron; N x 3\n",
    "# targ_vect = 2 x 1\n",
    "## OUTPUT\n",
    "# newFR = N x 1\n",
    "def brainFiringRate(lambda_vect, targ_vect):\n",
    "    targ_vect_mult = np.insert(targ_vect, 0, 1)\n",
    "    newFR = np.matmul(lambda_vect, targ_vect_mult)\n",
    "    return newFR[:, np.newaxis]\n",
    "\n",
    "#********\n",
    "# Passed test:\n",
    "# lambda_vect = np.array([[1, 2, 3], [4, 5, 6], [7,8,9], [10, 11, 12]])\n",
    "# targ_vect = np.array([1, 2])\n",
    "# brainFiringRate(lambda_vect, targ_vect) = b + Wt\n",
    "#********\n",
    "\n",
    "\n",
    "###################\n",
    "# Function that alters the lambda paramters (b, W) for the brain\n",
    "# B(t) = b + [Wx Wy][tx ty]' = firing rate\n",
    "# This is Equation (6) in Heliot et al, 2010\n",
    "\n",
    "## INPUT\n",
    "# N = number of neurons\n",
    "# lambda_vect = [b, W_x, W_y] for each neuron; N x 3\n",
    "# delta_perturb = N x 1  \n",
    "# targ_vect = 2 x 1\n",
    "## MIDDLE\n",
    "# targ_vect_mult = 3 x 1 [1 t_x t_y]'\n",
    "# targ_matx = N x 3\n",
    "# delta_matx = N x 3\n",
    "# next_term = delta_matx*delta_matx (element-wise mult) = N x 3\n",
    "## OUTPUT\n",
    "# lambda_vect_new = N x 3\n",
    "\n",
    "def calcNextLambda(lambda_vect, gamma, delta_perturb, targ_vect):\n",
    "    \n",
    "    # dimensions of neurons and lambda matrix\n",
    "    num_neurons = np.size(lambda_vect, 0)\n",
    "    num_lambda = np.size(lambda_vect, 1)\n",
    "    \n",
    "    # This is the vector to multiply the lambda update term with = [1 t_x t_y]'\n",
    "    targ_vect_mult = np.insert(targ_vect, 0, 1) # 3 x 1\n",
    "    \n",
    "    # Repeated the target vector NUM_NUERONS times to get a N x 3 matrix with the rows = [1 t_x t_y]\n",
    "    targ_matx = (np.matlib.repmat(targ_vect_mult, num_neurons, 1)) # N x 3\n",
    "    \n",
    "    # each neuron has a stochastic perturbation term that is used to multiply the target vector \n",
    "    # See Eq 6 of Heliot et al, 2010\n",
    "    delta_matx = np.matlib.repmat(delta_perturb, 1, num_lambda) # N x 3\n",
    "    \n",
    "    # Element-wise multiplication to find the update term for gamma\n",
    "    next_term = (delta_matx*targ_matx) # N x 3\n",
    "\n",
    "    # next gradient term: \\gamma*delta_perturb\n",
    "    lambda_vect_new = lambda_vect - (gamma*next_term)\n",
    "    \n",
    "     # NOTE: dimensions are going to be annoying here\n",
    "    if (KW_ONLY):\n",
    "        lambda_vect_new[:, 0] = 0\n",
    "        \n",
    "    return lambda_vect_new\n",
    "\n",
    "\n",
    "#********\n",
    "# Passed test:\n",
    "# lambda_vect = np.array([[1, 2, 3], [4, 5, 6], [7,8,9], [10, 11, 12]])\n",
    "# targ_vect = np.array([1, 2])\n",
    "# gamma = 0.5\n",
    "# delta_perturb = np.array([[4], [5], [6], [7]])\n",
    "# calcNextLambda(lambda_vect, gamma, delta_perturb, targ_vect) = lambda_vect - (gamma*error grad*[1 tx ty])\n",
    "#********\n",
    "\n",
    "# def calcNextBrain_old(brain_params, decoder_params, cost_func, targ_vect):\n",
    "    \n",
    "#     # Unpack arguments that are being passed in\n",
    "#     (fr_curr, fr_dist, lambda_curr, lambda_rate) = brain_params\n",
    "#     (a_vect, a_rate, a_dist, k_matx, k_rate, k_dist) = decoder_params\n",
    "#     decoder_vals = (a_vect, k_matx)\n",
    "#     # set cost function arguments\n",
    "#     # fr_curr, targ_vect, lambda_curr = brain_vars\n",
    "#     cost_func_args = ( (a_vect, k_matx), (fr_curr, targ_vect, lambda_curr))\n",
    "#     # set the vectors for the next value\n",
    "#     fr_next = np.zeros(np.shape(fr_curr))\n",
    "#     lambda_next = np.zeros(np.shape(lambda_curr))\n",
    "\n",
    "#      # (1) calculate the perturbation\n",
    "#     fr_grad = findErrorGrad(fr_curr, FR_VAR, fr_dist, cost_func, cost_func_args)\n",
    "    \n",
    "#     # (2) Find next lambda\n",
    "#     lambda_next = calcNextLambda(lambda_curr, lambda_rate, fr_grad, targ_vect)\n",
    "    \n",
    "#     # (3) fr+ = B(lambda+)\n",
    "#     fr_next = brainFiringRate(lambda_next, targ_vect)\n",
    "    \n",
    "#     return  (fr_next, lambda_next)\n",
    "\n",
    "\n",
    "# TODO: see if the baseline scalar value is affecting the cost function \n",
    "def calcNextBrain(brain_params, decoder_params, cost_func, targ_vect):\n",
    "    \n",
    "    # Unpack arguments that are being passed in\n",
    "    (fr_curr, fr_dist, lambda_curr, lambda_rate, l_dist) = brain_params\n",
    "    (a_vect, a_rate, a_dist, k_matx, k_rate, k_dist) = decoder_params\n",
    "\n",
    "    # set cost function arguments\n",
    "    cost_func_args = ( (a_vect, k_matx), (fr_curr, targ_vect, lambda_curr))\n",
    "    \n",
    "    # set the vectors for the next value\n",
    "    fr_next = np.zeros(np.shape(fr_curr))\n",
    "    lambda_next = np.zeros(np.shape(lambda_curr))\n",
    "\n",
    "     # (1) calculate the perturbation\n",
    "    lambda_grad = findErrorGrad(lambda_curr, L_VAR, l_dist, cost_func, cost_func_args)\n",
    "    \n",
    "    # (2) Find next lambda\n",
    "    lambda_next = lambda_curr - lambda_rate*lambda_grad\n",
    "    \n",
    "    # (3) fr+ = B(lambda+)\n",
    "    fr_next = brainFiringRate(lambda_next, targ_vect)\n",
    "        \n",
    "    if (KW_ONLY):\n",
    "        # set baseline to 0\n",
    "        lambda_next[:, 0] = 0\n",
    "        \n",
    "    return  (fr_next, lambda_next)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Decoder Model \n",
    "# Affine Function that calculates target postion from firing rate\n",
    "# Y = D(f) = a + Kf --> Weiner Filter\n",
    "# This is Equation (1) in Heliot et al, 2010\n",
    "\n",
    "## INPUT\n",
    "# N = number of neurons, d = dimension of target\n",
    "# decoder params = current a vector and K matrix (a = d x 1, K = d x N)\n",
    "# brain_params = fr_curr, target position, current lambda value\n",
    "# fr_curr = current firing rate as a vector\n",
    "# targ_vect = target position (T_x, Y_y)\n",
    "## OUTPUT\n",
    "# Cursor position = Y_x, Y_y\n",
    "def decoder_findY(decoder_params, brain_params):\n",
    "    # check firing rate \n",
    "    # un-nests the input values\n",
    "    (a_vect, k_matx) = decoder_params\n",
    "    (fr_curr, targ, lamdba_curr) = brain_params\n",
    "    \n",
    "    num_dim = np.shape(targ)\n",
    "    \n",
    "    # sets the dimensions for cursor position\n",
    "    cursor_pos = np.zeros(num_dim)\n",
    "    # reshape added to ensure that the a_vect is the correct dimensions\n",
    "    # Y = D(f) = a + Kf --> Weiner Filter\n",
    "    cursor_pos = a_vect.reshape(num_dim) +  (np.matmul(k_matx, fr_curr))\n",
    "    return (cursor_pos)\n",
    "\n",
    "#********\n",
    "# Passed Test: \n",
    "# a = np.array([-10, -20])\n",
    "# k = np.random.random([2, 4])\n",
    "# NUM_DIM = 2\n",
    "# fr = brainFiringRate(lambda_vect, targ_vect)\n",
    "# decoder_params = (a, k)\n",
    "# brain_params = (fr, targ_vect)\n",
    "# pos = decoder_findY(decoder_params, brain_params) = a + Kf\n",
    "#********\n",
    "\n",
    "DECODER_ADAPT = True\n",
    "\n",
    "# Function uses stochastic gradient descent to adjust decoder parameters\n",
    "## INPUT\n",
    "# decoder params = current a vector and K matrix (a = d x 1, K = d x N)\n",
    "# fr_curr = current firing rate as a vector\n",
    "## OUTPUT\n",
    "# next decoder parameters = a_next, k_next\n",
    "    \n",
    "def calcNextDecoder(decoder_params, brain_vars, cost_func):\n",
    "    \n",
    "    # un-nests parameters\n",
    "    (a_vect, a_rate, a_dist, k_matx, k_rate, k_dist) = decoder_params\n",
    "    cost_func_args = ( (a_vect, k_matx), brain_vars)\n",
    "    \n",
    "    if (DECODER_ADAPT == True):\n",
    "        # if decoder adaptation is true, update the A and K parameters of the brain\n",
    "        a_grad = findErrorGrad(a_vect, A_VAR, a_dist, cost_func, cost_func_args)\n",
    "        k_grad = findErrorGrad(k_matx, K_VAR, k_dist, cost_func, cost_func_args)\n",
    "        a_next = a_vect.reshape(np.shape(a_grad)) - a_rate*a_grad\n",
    "        k_next = k_matx - k_rate*k_grad\n",
    "    else:\n",
    "        # if DECODER_ADAPT == False (so no decoder adaptation), return the same a_vector and K_matrix\n",
    "        a_next = a_vect\n",
    "        k_next = k_matx\n",
    "        \n",
    "    if (KW_ONLY):\n",
    "        a_next = np.zeros(np.shape(a_vect))\n",
    "        \n",
    "    return (a_next, k_next)\n",
    "\n",
    "\n",
    "#*******\n",
    "# TODO: Test this function\n",
    "#*******\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reach Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joint cost = reach error + brain penalty cost + decoder penalty cost\n",
    "\n",
    "reach error = $||y-\\tau||^2 $\n",
    "\n",
    "brain cost = $||y-\\tau||^2  + \\sigma_{w}||W||^2$\n",
    "\n",
    "decoder cost = $||y-\\tau||^2 + \\sigma_{k}||K||^2$\n",
    "\n",
    "Joint cost = brain cost + decoder cost = 2$||y-\\tau||^2 + \\sigma_{w}||W||^2 + \\sigma_{k}||K||^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGMA = 1.2\n",
    "# sigma/variance of target < 1\n",
    "# sigma/(target^2) < 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Reach error\n",
    "# This is Equation (3) in Heliot et al, 2010\n",
    "\n",
    "## INPUT\n",
    "# y_x, y_y = predicted cursor position\n",
    "# t_x, t_y = target position\n",
    "## OUTPUT\n",
    "# norm squared of (target position - cursor position)\n",
    "# this returns the square of distance between the two position points\n",
    "# where reach error is the target position - cursor position\n",
    "def calcReachError(y_vect, t_vect):\n",
    "    norm_vect = np.array(y_vect).reshape(NUM_DIM,1) - np.array(t_vect).reshape(NUM_DIM, 1)\n",
    "    return (np.linalg.norm(norm_vect, 2)**2)\n",
    "\n",
    "#*******\n",
    "# Failed test:\n",
    "# Had to add .reshape(NUM_DIM, 1) to the norm vect\n",
    "# Update:\n",
    "# Passed Test\n",
    "#*******\n",
    "\n",
    "## INPUT\n",
    "# cost_func_params = decoder params (a vect, k matx) and current firing rate\n",
    "## OUTPUT\n",
    "# reach error = scalar; norm squared of (target position - cursor position)\n",
    "# where reach error is the target position - cursor position\n",
    "def reach_cost(cost_func_params):\n",
    "    (decoder_params, brain_vars) = cost_func_params\n",
    "    (fr_curr, targ_vect, lambda_curr) = brain_vars\n",
    "    y_vect = decoder_findY(decoder_params, brain_vars)\n",
    "    t_vect = targ_vect.copy()\n",
    "    return calcReachError(y_vect, t_vect)  \n",
    "\n",
    "def brain_cost(cost_func_params):\n",
    "    # parameters\n",
    "    sig_b = SIGMA #sigma for brain matrix W\n",
    "    \n",
    "    # un-pack params to get W\n",
    "    (decoder_params, brain_vars) = cost_func_params\n",
    "    (fr_curr, targ_vect, lambda_curr) = brain_vars\n",
    "    \n",
    "    # calc brain regularization term\n",
    "    w_matx = lambda_curr[:, 1:NUM_LAMBDA]\n",
    "    brain_penalty = sig_b*(np.linalg.norm(w_matx,2)**2)\n",
    "    \n",
    "    # add brain penalty to reach error\n",
    "    cost = reach_cost(cost_func_params) + brain_penalty\n",
    "    return cost \n",
    "\n",
    "def decoder_cost(cost_func_params):\n",
    "    # parameters\n",
    "    sig_k = SIGMA #sigma for decoder matrix K\n",
    "\n",
    "    # un-pack params to get K \n",
    "    (decoder_params, brain_vars) = cost_func_params\n",
    "    (a_vect, k_matx) = decoder_params\n",
    "    \n",
    "    # calc decoder regularization term\n",
    "    dec_penalty = sig_k*(np.linalg.norm(k_matx,2)**2)\n",
    "    cost = reach_cost(cost_func_params) + dec_penalty \n",
    "    return cost   \n",
    "\n",
    "def joint_cost(cost_func_params):\n",
    "     # parameters\n",
    "    sig_b = SIGMA #sigma for brain matrix W\n",
    "    sig_k = SIGMA #sigma for decoder matrix K\n",
    "    \n",
    "    # un-pack params to get K and W\n",
    "    (decoder_params, brain_vars) = cost_func_params\n",
    "    (fr_curr, targ_vect, lambda_curr) = brain_vars\n",
    "    (a_vect, k_matx) = decoder_params\n",
    "    \n",
    "    # brain penalty term\n",
    "    w_matx = lambda_curr[:, 1:NUM_LAMBDA]\n",
    "    brain_cost = sig_b*(np.linalg.norm(w_matx,2)**2)\n",
    "    \n",
    "    # decoder penalty term\n",
    "    dec_cost = sig_k*(np.linalg.norm(k_matx,2)**2)\n",
    "  \n",
    "    return reach_cost(cost_func_params) + brain_cost + dec_cost\n",
    "\n",
    "## NOTES ABOUT THIS\n",
    "# 09/20/2020: confirmed that the np.linalg.norm(matx, 2) returns the magnitude -- anyway, neither matrix is ever <0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Error Descent: Update Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "## Stochastic Error Descent\n",
    "# This is Equation (4) and Equation (5) in Heliot, 2010\n",
    "# Derivation is found in Cauwenberghs, 1993 \n",
    "\n",
    "# This function is one iteration of the error descent calcuation\n",
    "## INPUT\n",
    "# input_vect = vector to stochastically perturb\n",
    "# input_var = FR_VAR, A_VAR OR K_VAR \n",
    "# param_dist = distribution from which perturbations to the input are selected\n",
    "# cost_func = cost function (always reach error)\n",
    "# cost_func_args = arguments to the cost function (depends on error cost function)\n",
    "## OUTPUT\n",
    "# errorGrad = gradient for updating parameter (input vect)\n",
    "def findErrorGrad(input_vect, input_var, param_dist, cost_func, cost_func_args):\n",
    "    # Un-nest everything\n",
    "    (sigma, delta, num_dist) = param_dist\n",
    "    (decoder_params, (curr_fr, targ_vect, curr_lambda) ) = cost_func_args\n",
    "    (a_vect, k_matx) = decoder_params\n",
    "    \n",
    "    # Get size of input vector\n",
    "    num_neurons = np.size(k_matx, 1)\n",
    "    num_input_row = np.size(input_vect, 0) \n",
    "    num_input_column = 1\n",
    "    if (input_vect.ndim > 1): \n",
    "        num_input_column = np.size(input_vect, 1)\n",
    "    \n",
    "    # What to perturb and input firing rate for error cost function\n",
    "    input_vect = input_vect.copy().reshape(num_input_row, num_input_column)\n",
    "    input_fr = curr_fr.copy().reshape(num_neurons, 1)\n",
    "       \n",
    "    error_sum = np.zeros((num_input_row, 1)) \n",
    "    error_grad = np.zeros((num_input_row, 1))\n",
    "#     perturb_rand = np.random.uniform(-sigma, sigma, [num_input_row, num_input_column, num_dist])\n",
    "    perturb_rand = np.random.normal(0, sigma, [num_input_row, num_input_column, num_dist])\n",
    "\n",
    "# Normalizes the stochastic perturbations to 1 and then multiplies by sigma, so perturb_rand = +/- sigma\n",
    "#     perturb_rand /= np.linalg.norm(perturb_rand,axis=1)[:,np.newaxis]\n",
    "#     perturb_rand *= sigma\n",
    "\n",
    "    for iD in range(num_dist):\n",
    "        # perturb_vect = stochastic pertrbation (amount of stochastic descent perturbation)\n",
    "        perturb_vect = np.squeeze(perturb_rand[:, :, [iD]])\n",
    "        perturb_vect = perturb_vect.copy().reshape(num_input_row, num_input_column)\n",
    "        \n",
    "        # find the delta error caused by the perturbation (direction to descend gradient in) \n",
    "        input_perturb = np.add(input_vect, delta*perturb_vect) \n",
    "        \n",
    "        # Case 1: firing rate\n",
    "        # error = reachError(a + K*fr')\n",
    "        if (input_var == FR_VAR):        \n",
    "            # Calculate error \n",
    "            perturb_cost_args = (decoder_params, (input_perturb, targ_vect, curr_lambda) ) \n",
    "            \n",
    "        # case 2: a\n",
    "        # error = reachError(a' + K*fr)\n",
    "        elif (input_var == A_VAR):\n",
    "            decoder_params_perturb = (input_perturb, k_matx)\n",
    "            perturb_cost_args = (decoder_params_perturb, (input_fr, targ_vect, curr_lambda))\n",
    "\n",
    "        # case 3: k\n",
    "        # error = reachError(a + K'*fr)\n",
    "        elif (input_var == K_VAR):\n",
    "            decoder_params_perturb = (a_vect, input_perturb)\n",
    "            perturb_cost_args = (decoder_params_perturb, (input_fr, targ_vect, curr_lambda))\n",
    "        \n",
    "        # default: do nothing\n",
    "        elif (input_var == L_VAR):\n",
    "            if (KW_ONLY):\n",
    "                input_perturb[:, 0] = 0\n",
    "            fr_perturb = brainFiringRate(input_perturb, targ_vect)\n",
    "            perturb_cost_args = (decoder_params, (fr_perturb, targ_vect, input_perturb))\n",
    "            \n",
    "        else:\n",
    "            perturb_cost_args = cost_func_args\n",
    "        \n",
    "        error_perturb = cost_func(perturb_cost_args)   \n",
    "        error_input = cost_func(cost_func_args)\n",
    "        error_sum = np.add(error_sum, (error_perturb - error_input)*perturb_vect)\n",
    "\n",
    "#     error_grad = np.array(error_sum/(num_dist*delta*(sigma**2)))\n",
    "    error_grad = np.array(error_sum/(num_dist*delta))\n",
    "    return error_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent: Update Step and Recalculate Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate New Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def findNextTarget(curr_cursor, prev_targ):\n",
    "#     ## random\n",
    "# #     x_pos = 2*np.random.random_sample()\n",
    "# #     y_pos = 2*np.random.random_sample()\n",
    "#     x_pos = 1\n",
    "#     y_pos = 1\n",
    "\n",
    "#     return np.array( [x_pos, y_pos] )\n",
    "\n",
    "def findNextTarget(curr_cursor, prev_targ):\n",
    "    ## random\n",
    "#     x_pos = 2*np.random.random_sample()\n",
    "    x_pos = 1\n",
    "    if np.random.random() < 0.5:\n",
    "        x_pos = -1\n",
    "\n",
    "    return np.array( [x_pos] )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Side Note:\n",
    "\n",
    "Almost all module functions depend on the basic function random(), which generates a random float uniformly in the semi-open range [0.0, 1.0). Python uses the Mersenne Twister as the core generator. It produces 53-bit precision floats and has a period of 2**19937-1. The underlying implementation in C is both fast and threadsafe. The Mersenne Twister is one of the most extensively tested random number generators in existence. However, being completely deterministic, it is not suitable for all purposes, and is completely unsuitable for cryptographic purposes.\n",
    "\n",
    "From: https://docs.python.org/2/library/random.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set initial conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "## Set some initial conditions here\n",
    "\n",
    "# Helper info:\n",
    "# fr_init = N x 1\n",
    "# lambda init = N x 3\n",
    "# baseline = 1 x N\n",
    "# target_vector = 2 x 1\n",
    "# K_matx = 2 x N\n",
    "# A = 2 x 1\n",
    "\n",
    "KW_ONLY = True #False = include A/B, True = only K/W, no baseline\n",
    "\n",
    "# ## Save initial conditions\n",
    "filename = str(today) + '_INITIAL_CONDITIONS.txt'\n",
    "print(filename)\n",
    "stdout_fileno = sys.stdout\n",
    "sys.stdout = open(filename, \"w\")\n",
    "    \n",
    "NUM_NEURONS = 10\n",
    "NUM_DIM = 1\n",
    "NUM_LAMBDA = NUM_DIM + 1\n",
    "\n",
    "print(\"=========================================\")\n",
    "print(\"INITIAL CONDITIONS\")\n",
    "print(\"=========================================\")\n",
    "\n",
    "print(\"NUM NEURONS = \" + str(NUM_NEURONS))\n",
    "# target position\n",
    "# TARGET_VECTOR = np.array([[1] , [1]])\n",
    "TARGET_VECTOR = np.array([[1]])\n",
    "print(\"dimensions of target = \" + str(np.shape(TARGET_VECTOR)))\n",
    "print(\"initial target = \" + str(TARGET_VECTOR))\n",
    "\n",
    "# firing rate \n",
    "fr_init = np.zeros( (NUM_NEURONS, 1) ) \n",
    "\n",
    "# BASELINE (b)\n",
    "BASELINE = (0*np.random.random_sample(NUM_NEURONS))  # random float [0, 10)\n",
    "print(\"baseline shape = \" + str(np.shape(BASELINE)))\n",
    "print(\"b =\" + str(BASELINE))\n",
    "\n",
    "# decoder initial paramters\n",
    "# IDEAL: y = a + Kf = a + K(b + Wt) = a + Kb + KWt\n",
    "# in order for y = t, want: a + Kb --> 0 and KW --> Identity matx\n",
    "K_MATX = np.random.random_sample((NUM_DIM, NUM_NEURONS)) # random float [0, 1)\n",
    "# K_MATX = (1)*np.ones((NUM_DIM, NUM_NEURONS)) # random float [0, 1)\n",
    "# A_VECT = (-np.matmul(K_MATX, BASELINE))\n",
    "A_VECT = -np.random.random_sample((NUM_DIM))*0\n",
    "print(\"K MATX = \" + str(K_MATX))\n",
    "print(\"A = \" + str(A_VECT))\n",
    "\n",
    "# lambda\n",
    "lambda_init = np.zeros((NUM_NEURONS, NUM_LAMBDA))\n",
    "# W_init = 0*np.linalg.pinv(K_MATX) \n",
    "# W_rand = np.random.random_sample( (NUM_NEURONS, NUM_DIM) )\n",
    "# W_init = W_init + W_rand\n",
    "W_init = np.random.random_sample((NUM_NEURONS, NUM_DIM))\n",
    "# W_init = (1)*np.ones((NUM_NEURONS, NUM_DIM))\n",
    "lambda_init[:, 0] = np.array([BASELINE])        # lambda[0] = baseline\n",
    "lambda_init[:, 1:NUM_LAMBDA] = W_init\n",
    "print(\"lambda init = \")\n",
    "print(lambda_init)\n",
    "\n",
    "\n",
    "# lambda_init[:, 1] = np.random.random_sample(np.shape(lambda_init[:, 0]))    # lambda[2] = w_y\n",
    "# lambda_init[:, 2] = np.random.random_sample(np.shape(lambda_init[:, 0]))*10 \n",
    "# # lambda_init[:, 1] = np.array([0.4, 0.6, 1, 2])  # lambda[1] = w_x\n",
    "# # lambda_init[:, 2] = np.array([3, 5, 4, 2])      # lambda[2] = w_\n",
    "\n",
    "# SGD initial parameters\n",
    "# Mapping for variables\n",
    "FR_VAR = 1\n",
    "A_VAR = 2\n",
    "K_VAR = 3\n",
    "L_VAR = 4\n",
    "\n",
    "SIGMA_GAUSSIAN = 1\n",
    "SGD_DELTA = 1e-2\n",
    "\n",
    "# Brain\n",
    "FR_SIGMA = SIGMA_GAUSSIAN\n",
    "FR_DELTA = SGD_DELTA\n",
    "FR_DIST_SIZE = 1\n",
    "print(\"\")\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"FR DISTRIBUTION\")\n",
    "print(\"FR_SIGMA = \" + str(FR_SIGMA))\n",
    "print(\"FR_DELTA = \" + str(FR_DELTA))\n",
    "print(\"FR_DIST_SIZE = \" + str(FR_DIST_SIZE))\n",
    "\n",
    "\n",
    "L_SIGMA = SIGMA_GAUSSIAN\n",
    "L_DELTA = SGD_DELTA\n",
    "L_DIST_SIZE = 1\n",
    "print(\"\")\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"LAMBDA DISTRIBUTION\")\n",
    "print(\"L_SIGMA = \" + str(FR_SIGMA))\n",
    "print(\"L_DELTA = \" + str(FR_DELTA))\n",
    "print(\"L_DIST_SIZE = \" + str(FR_DIST_SIZE))\n",
    "\n",
    "\n",
    "# Decoder\n",
    "# A_RATE = 1e-4\n",
    "A_SIGMA = SIGMA_GAUSSIAN\n",
    "A_DELTA = SGD_DELTA\n",
    "A_DIST_SIZE = 1\n",
    "print(\"\")\n",
    "print(\"A DISTRIBUTION\")\n",
    "print(\"A_SIGMA = \" + str(A_SIGMA))\n",
    "print(\"A_DELTA = \" + str(A_DELTA))\n",
    "print(\"A_DIST_SIZE = \" + str(A_DIST_SIZE))\n",
    "# --\n",
    "# K_RATE = 5e-4\n",
    "K_SIGMA = SIGMA_GAUSSIAN\n",
    "K_DELTA = SGD_DELTA\n",
    "K_DIST_SIZE = 1\n",
    "print(\"\")\n",
    "print(\"K DISTRIBUTION\")\n",
    "print(\"K_SIGMA = \" + str(K_SIGMA))\n",
    "print(\"K_DELTA = \" + str(K_DELTA))\n",
    "print(\"K_DIST_SIZE = \" + str(K_DIST_SIZE))\n",
    "# display parameters\n",
    "# fig_x = 10\n",
    "# fig_y = 5\n",
    "\n",
    "# cost function\n",
    "JOINT_COST = joint_cost\n",
    "BRAIN_COST = brain_cost\n",
    "DECODER_COST = decoder_cost\n",
    "\n",
    "sys.stdout.close()\n",
    "## Restore sys.stdout to our old saved file handler\n",
    "sys.stdout = stdout_fileno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BRAIN_ADAPT = True\n",
    "# SIGMA_GAUSSIAN = 1e-1\n",
    "\n",
    "NUM_TRIALS = 5000\n",
    "\n",
    "# NUM_RUNS = 2 # Has to be >1 # 2 = goes through the trial for target once\n",
    "NUM_SESSIONS = 1\n",
    "rate_list = [5e-3]\n",
    "# rate_list = [3e-4]\n",
    "print(rate_list)\n",
    "# dec_rate_list = (np.logspace(-2, -6, 11))\n",
    "dec_rate_list = rate_list\n",
    "# K_RATE = 1e-4\n",
    "NUM_B_RATE = len(rate_list)\n",
    "print(\"NUM_B_RATE = \" + str(NUM_B_RATE))\n",
    "NUM_D_RATE = len(dec_rate_list)\n",
    "print(\"NUM_D_RATE = \" + str(NUM_D_RATE))\n",
    "ADAPT_TRIALS = NUM_TRIALS\n",
    "\n",
    "# initialization\n",
    "cursor_start = np.zeros( (NUM_DIM, 1, NUM_TRIALS, NUM_SESSIONS) )\n",
    "cursor_end = np.zeros( (NUM_DIM, 1, NUM_TRIALS, NUM_SESSIONS) )\n",
    "# Note to self: last NUM_TRIALS element of target_trial is not run\n",
    "target_trial = np.zeros( (NUM_DIM, 1, NUM_TRIALS, NUM_SESSIONS) )\n",
    "lambda_trial = np.zeros( (NUM_NEURONS, NUM_LAMBDA, NUM_TRIALS, NUM_SESSIONS) )\n",
    "fr_trial = np.zeros( (NUM_NEURONS, 1, NUM_TRIALS) )\n",
    "a_trial = np.zeros( (NUM_DIM, NUM_TRIALS, NUM_SESSIONS) )\n",
    "k_trial = np.zeros( (NUM_DIM, NUM_NEURONS, NUM_TRIALS, NUM_SESSIONS) )\n",
    "cost_startT = np.zeros( (NUM_TRIALS, NUM_SESSIONS) )\n",
    "cost_endT = np.zeros( (NUM_TRIALS, NUM_SESSIONS) )\n",
    "b_cost = np.zeros( (NUM_TRIALS, NUM_SESSIONS) )\n",
    "d_cost = np.zeros( (NUM_TRIALS, NUM_SESSIONS) )\n",
    "re_cost = np.zeros((NUM_TRIALS, NUM_SESSIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveExperimentToFile(filename):\n",
    "    ## SAVE README\n",
    "    \n",
    "    stdout_fileno = sys.stdout\n",
    "    sys.stdout = open(filename + '_README.txt', \"w\")\n",
    "\n",
    "    print(\"=========================================\")\n",
    "    print(\"EXPERIMENT DETAILS\")\n",
    "    print(\"=========================================\")\n",
    "    print(\"Adaptive Decoder = \" + str(DECODER_ADAPT))\n",
    "    print(\"Adaptive Brain = \" + str(BRAIN_ADAPT))\n",
    "    print(\"KW only = \" + str(KW_ONLY))\n",
    "    print(\"Total Number of Trials = \" + str(NUM_TRIALS))\n",
    "    print(\"Total Number of Sessions = \" + str(NUM_SESSIONS))\n",
    "#     print(\"Total Number of Runs = \" + str(NUM_RUNS))\n",
    "    print(\"Brain Learning Rate = \" + str(lambda_rate))\n",
    "    print(\"NUM_B_RATE = \" + str(NUM_B_RATE))\n",
    "    print(\"Decoder (K) Learning Rate = \" + str(K_RATE))\n",
    "    print(\"Decoder (A) Learning Rate = \" + str(A_RATE))\n",
    "    print(\"NUM_D_RATE = \" + str(NUM_D_RATE))\n",
    "    print(\"=========================================\")\n",
    "\n",
    "    sys.stdout.close()\n",
    "    # Restore sys.stdout to our old saved file handler\n",
    "    sys.stdout = stdout_fileno\n",
    "\n",
    "    ## SAVE PARAMETERS\n",
    "    np.savez(filename + '_Parameters.npz', num_trials=NUM_TRIALS, num_sessions=NUM_SESSIONS, \n",
    "     num_neurons=NUM_NEURONS, num_dim=NUM_DIM, num_lambda=NUM_LAMBDA,\n",
    "     decoder_adapt=DECODER_ADAPT, brain_adapt=BRAIN_ADAPT, brain_rate=lambda_rate, \n",
    "     k_rate=K_RATE, a_rate=A_RATE, num_b_rate=NUM_B_RATE, num_d_rate=NUM_D_RATE, b_rate_list=rate_list, d_rate_list=dec_rate_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pidx = 0\n",
    "n_fig = 5\n",
    "for lambda_rate in rate_list:\n",
    "    for dec_rate in dec_rate_list:\n",
    "        A_RATE = dec_rate\n",
    "        K_RATE = dec_rate    \n",
    "        \n",
    "        datafile = str(today) + '_BRAIN_ADAPT_' + str(BRAIN_ADAPT) + '_BRAIN_RATE_' + str(lambda_rate) + '_DEC_ADAPT_'+ str(DECODER_ADAPT) + '_K_RATE_' + str(dec_rate) + '_A_RATE_' + str(dec_rate)\n",
    "        saveExperimentToFile(datafile)\n",
    "\n",
    "        for iS in range(NUM_SESSIONS):\n",
    "            print(\"\")\n",
    "            print(\"+++++++++++++++++++++++++++++++++++\")\n",
    "            print(\"Session #\" + str(iS))\n",
    "            print(\"Session Start\")\n",
    "            print(\"brain rate = \" + str(lambda_rate))\n",
    "            print(\"decoder rate = \" + str(dec_rate))\n",
    "            ## BRAIN SIDE\n",
    "            FR_DIST = (FR_SIGMA, FR_DELTA, FR_DIST_SIZE)\n",
    "            L_DIST = (L_SIGMA, L_DELTA, L_DIST_SIZE)\n",
    "\n",
    "            ## DECODER SIDE\n",
    "            A_DIST = (A_SIGMA, A_DELTA, A_DIST_SIZE)\n",
    "            K_DIST = (K_SIGMA, K_DELTA, K_DIST_SIZE)\n",
    "\n",
    "            # target position -- new target represents a new trial\n",
    "            target_trial[:, :, 0, iS] = TARGET_VECTOR\n",
    "            \n",
    "            ## VECTORS FOR TRIALS    \n",
    "            lambda_trial[:, :, 0, iS] = lambda_init\n",
    "            a_trial[:, 0, iS] = np.array(A_VECT)\n",
    "            k_trial[:,:, 0, iS] = K_MATX\n",
    "            decoder_params = (a_trial[:, 0, iS], A_RATE, A_DIST, k_trial[:,:,  0, iS], K_RATE, K_DIST)\n",
    "            decoder_vals = (A_VECT, K_MATX)\n",
    "\n",
    "            for iT in range(NUM_TRIALS-1):\n",
    "#                 print(\"\")\n",
    "#                 print(\"=========================================\")\n",
    "#                 print(\"Trial #\" + str(iT) + \" | lambda learn rate = \" + str(lambda_rate))\n",
    "#                 print(\"Target = \" + str(target_trial[:, :, iT, iS]))\n",
    "\n",
    "                # calculate firing rate given lambda and decoder parameters with current target position \n",
    "                fr_start = np.array(brainFiringRate(lambda_trial[:, :, iT, iS], target_trial[:, :, iT, iS]))\n",
    "                fr_trial[:, :, iT] = fr_start\n",
    "\n",
    "                # calculate reach error of firing rate at the beginning of the trial\n",
    "                # this becomes the error of the new target position being presented and where the cursor is\n",
    "                brain_vars = ( fr_trial[:, :, iT],  target_trial[:, :, iT, iS], lambda_trial[:, :, iT, iS] )\n",
    "                cost_func_params = (decoder_vals, brain_vars) \n",
    "\n",
    "                cost_startT[iT, iS] = JOINT_COST(cost_func_params)\n",
    "\n",
    "                cursor_start[ :, :, iT, iS] =  (decoder_findY(decoder_vals, brain_vars))\n",
    "\n",
    "                # Run through trial and see the reach at the end        \n",
    "                # current brain and decoder params\n",
    "                brain_params = (fr_trial[:, :, iT], FR_DIST, lambda_trial[:, :, iT, iS], lambda_rate, L_DIST)\n",
    "                decoder_params = (a_trial[:, iT, iS], A_RATE, A_DIST, k_trial[:, :,  iT, iS], K_RATE, K_DIST)\n",
    "\n",
    "                # adapt brain and decoder (together here)\n",
    "                a_run, k_run = calcNextDecoder(decoder_params, brain_vars, DECODER_COST)\n",
    "                \n",
    "                if (BRAIN_ADAPT):\n",
    "                    fr_run, lambda_run = calcNextBrain(brain_params, decoder_params, BRAIN_COST, target_trial[:, :, iT, iS])\n",
    "                else:\n",
    "                    fr_run = fr_trial[:, :, iT]\n",
    "                    lambda_run = lambda_trial[:, :, iT, iS]\n",
    "                    \n",
    "                if (KW_ONLY):\n",
    "                    a_run = np.zeros((NUM_DIM, 1))\n",
    "                    lambda_run[:, 0] = 0\n",
    "\n",
    "                # update cost function arguments\n",
    "                decoder_vals = (a_run, k_run)\n",
    "                brain_vars = (fr_run, target_trial[:, :, iT, iS], lambda_run)\n",
    "                # see how the updated decoder and brain paramters have done with the current\n",
    "                # target position (so target at trial = iT)\n",
    "                cost_func_params = (decoder_vals, brain_vars) \n",
    "#                 cost_run = np.array(JOINT_COST(cost_func_params))\n",
    "                cost_run = JOINT_COST(cost_func_params)\n",
    "                    \n",
    "                cursor_end[:, :, iT, iS] =  (decoder_findY(decoder_vals, brain_vars))\n",
    "\n",
    "                cost_endT[iT, iS] = cost_run\n",
    "                b_cost[iT, iS] = BRAIN_COST(cost_func_params)\n",
    "                d_cost[iT, iS] = DECODER_COST(cost_func_params)\n",
    "                re_cost[iT, iS] = reach_cost(cost_func_params)\n",
    "\n",
    "                # update the parameters\n",
    "                lambda_trial[:, :, iT + 1, iS] = np.squeeze(lambda_run)\n",
    "\n",
    "                a_trial[:, iT + 1, iS] = np.squeeze(a_run)\n",
    "                k_trial[:, :, iT + 1, iS] = (k_run)\n",
    "                \n",
    "                # change to new target\n",
    "                target_trial[:, 0, iT + 1, iS] = findNextTarget( cursor_end[:, :, iT, iS], target_trial[:, :, iT, iS] )\n",
    "\n",
    "            plt.figure(n_fig*pidx + 1) #, figsize=(fig_x, fig_y))\n",
    "            plt.plot(np.arange(0, NUM_TRIALS-1, 1), cost_startT[0:len(cost_startT)-1, iS], label = '' + str(iS))\n",
    "            plt.legend()\n",
    "            plt.xlabel('Trials')\n",
    "            plt.ylabel('Cost Function at the Start of Trial')\n",
    "            plt.title('System Cost across trials, brain rate = ' + str(lambda_rate) + \", decoder rate = \" + str(dec_rate))\n",
    "            \n",
    "            plt.figure(n_fig*pidx + 2) #, figsize=(fig_x, fig_y))\n",
    "            plt.plot(np.arange(0, NUM_TRIALS-1, 1), b_cost[0:len(cost_startT)-1, iS], label = '' + str(iS))\n",
    "            plt.legend()\n",
    "            plt.xlabel('Trials')\n",
    "            plt.ylabel('Brain Cost Function at the Start of Trial')\n",
    "            plt.title('Brain Cost across trials, brain rate = ' + str(lambda_rate) + \", decoder rate = \" + str(dec_rate))\n",
    "            \n",
    "            plt.figure(n_fig*pidx + 3) #, figsize=(fig_x, fig_y))\n",
    "            plt.plot(np.arange(0, NUM_TRIALS-1, 1), d_cost[0:len(cost_startT)-1, iS], label = '' + str(iS))\n",
    "            plt.legend()\n",
    "            plt.xlabel('Trials')\n",
    "            plt.ylabel('Decoder Cost Function at the Start of Trial')\n",
    "            plt.title('Decoder Cost across trials, brain rate = ' + str(lambda_rate) + \", decoder rate = \" + str(dec_rate))\n",
    "            \n",
    "            plt.figure(n_fig*pidx + 4) #, figsize=(fig_x, fig_y))\n",
    "            plt.plot(np.arange(0, NUM_TRIALS-1, 1), re_cost[0:len(cost_startT)-1, iS], label = '' + str(iS))\n",
    "            plt.legend()\n",
    "            plt.xlabel('Trials')\n",
    "            plt.ylabel('Reach Error Cost Function at the Start of Trial')\n",
    "            plt.title('Reach Error Cost across trials, brain rate = ' + str(lambda_rate) + \", decoder rate = \" + str(dec_rate))\n",
    "        \n",
    "            plt.figure(n_fig*pidx + 5)#, figsize=(fig_x, fig_y))\n",
    "            plt.plot(np.arange(0, NUM_TRIALS-1, 1), cost_endT[0:len(cost_startT)-1, iS]- re_cost[0:len(cost_startT)-1, iS], label = '' + str(iS))\n",
    "            plt.legend()\n",
    "            plt.xlabel('Trials')\n",
    "            plt.ylabel('Cost - Reach Error Function at the Start of Trial')\n",
    "            plt.title('Cost - Reach Error across trials, brain rate = ' + str(lambda_rate) + \", decoder rate = \" + str(dec_rate))\n",
    "        \n",
    "            print(\"\")\n",
    "            print(\"--------------------\")\n",
    "            print(\"Session #\" + str(iS))\n",
    "            print(\"Session End\")\n",
    "        \n",
    "        # to get the right trendlines to the right figure\n",
    "        pidx = pidx + 1 \n",
    "        \n",
    "        \n",
    "        ## SAVE DATA\n",
    "        # k_trial\n",
    "        save(datafile + '_Decoder_K_data.npy', k_trial)\n",
    "        # a_trial\n",
    "        save(datafile + '_Decoder_A_data.npy', a_trial)\n",
    "        # lambda_trial\n",
    "        save(datafile + '_Brain_lambda_data.npy', lambda_trial)\n",
    "        # cursor_start\n",
    "        save(datafile + '_Trial_cursor_start_data.npy', cursor_start)\n",
    "        # cursor_end\n",
    "        save(datafile + '_Trial_cursor_end_data.npy', cursor_end)\n",
    "        # target_trial\n",
    "        save(datafile + '_Trial_target_data.npy', target_trial)\n",
    "        # reach error\n",
    "        save(datafile + '_Trial_cost_start_data.npy', cost_startT)\n",
    "        save(datafile + '_Trial_cost_end_data.npy', cost_endT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(cursor_start)\n",
    "np.shape(np.mean(cursor_start, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "y = 1\n",
    "axm = NUM_DIM+2\n",
    "\n",
    "plt.figure(figsize = (12, 8))\n",
    "start_x_mean = np.mean(cursor_start, axis=axm)[x, 0, 0:NUM_TRIALS-1]\n",
    "end_x_mean = np.mean(cursor_end, axis=axm)[x, 0, 0:NUM_TRIALS-1]\n",
    "targ_x_mean = np.mean(target_trial, axis=axm)[x, 0, 0:NUM_TRIALS-1]\n",
    "\n",
    "plt.plot(start_x_mean, linestyle = '-', marker = '.', label = 'cursor (start of trial)')\n",
    "plt.plot(targ_x_mean, linestyle = '-', marker = '.', label = 'target' )\n",
    "plt.legend()\n",
    "plt.xlabel('Trials')\n",
    "plt.ylabel('Starting Cursor and Target Position (X)')\n",
    "plt.title('Mean Cursor Position (X) Across Trials',)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_num = 70\n",
    "plt.figure(figsize = (8, 8))\n",
    "plt.plot(targ_x_mean[0:show_num], linestyle = '', marker = '.', markersize = 10, label = 'target' )\n",
    "plt.plot(start_x_mean[0:show_num], linestyle = '', marker = '.', markersize = 10, label = 'cursor (start of trial)')\n",
    "plt.legend()\n",
    "plt.xlabel('Cursor and Target Position (X)')\n",
    "plt.ylabel('Cursor and Target Position (Y)')\n",
    "plt.title('Cursor and Target Position Across First ' + str(show_num) + ' Trials')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "plt.plot(targ_x_mean[-show_num:], linestyle = '', marker = '.', markersize = 10, label = 'target' )\n",
    "plt.plot(start_x_mean[-show_num:], linestyle = '', marker = '.', markersize = 10, label = 'cursor (start of trial)')\n",
    "plt.legend()\n",
    "plt.xlabel('Cursor and Target Position (X)')\n",
    "plt.ylabel('Cursor and Target Position (Y)')\n",
    "plt.title('Cursor and Target Position Across Last ' + str(show_num) + ' Trials')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "plt.plot(np.abs(start_x_mean - targ_x_mean), linestyle = '-', marker = '.', label = 'X error')\n",
    "# plt.plot(start_y_mean - targ_y_mean, linestyle = '-', marker = '.', label = 'Y error' )\n",
    "plt.legend()\n",
    "plt.xlabel('Trials')\n",
    "plt.ylabel('Difference in Starting Cursor and Target Position')\n",
    "plt.title('Mean Cursor Error (Absolute Value) Across Trials')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(k_trial[:, :, 3, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(lambda_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(lambda_trial[:, NUM_LAMBDA-1, 3].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_inner = np.zeros((NUM_NEURONS, NUM_TRIALS))\n",
    "for iT in range(NUM_TRIALS):\n",
    "    k_T = k_trial[:, :, iT, 0]\n",
    "    w_T = np.transpose(lambda_trial[:, NUM_LAMBDA - 1, iT])\n",
    "#     kw_inner[iT] = (np.inner(k_T, w_T))/(np.linalg.norm(k_T, 2)*np.linalg.norm(w_T, 2))\n",
    "    kw_inner[:, iT] = k_T/w_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lk_mtx = np.zeros((NUM_DIM, NUM_DIM, NUM_TRIALS))\n",
    "lk_mtx_temp = np.zeros((NUM_DIM, NUM_DIM, NUM_TRIALS, NUM_SESSIONS))\n",
    "# print(np.shape((k_trial[:,:, :, 1])))\n",
    "# print((lambda_trial[:, 1:3, 1, 1]))\n",
    "for iT in range(NUM_TRIALS):\n",
    "    for iS in range(NUM_SESSIONS):\n",
    "        lk_mtx_temp[:,:, iT, iS] = np.matmul(k_trial[:,:, iT, iS], lambda_trial[:, 1:3, iT, iS])\n",
    "    lk_mtx = np.mean(lk_mtx_temp[:,:, :, :], axis=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(lk_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize = (12, 8))\n",
    "plt.plot(lk_mtx[0, 0, :])\n",
    "plt.xlabel(\"Trials\")\n",
    "plt.title(\"Mean KW\")\n",
    "\n",
    "plt.figure(figsize = (12, 8))\n",
    "plt.plot(np.log10(lk_mtx[0, 0, :]))\n",
    "plt.xlabel(\"Trials\")\n",
    "plt.title(\"Log-10 of Mean KW\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last = 70\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lk_mtx[0, 0, NUM_TRIALS-1-last:NUM_TRIALS-1])\n",
    "plt.xlabel(\"Trials\")\n",
    "plt.title(\"Mean KW  | Last \" + str(last) + \" Trials \")\n",
    "\n",
    "last_kw_mean = np.mean(lk_mtx[0, 0, NUM_TRIALS-last-1: NUM_TRIALS-1])\n",
    "print(\"Mean of Last KW = \"+ str(last_kw_mean))\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.log10(lk_mtx[0, 0, NUM_TRIALS-1-last:NUM_TRIALS-1]))\n",
    "plt.xlabel(\"Trials\")\n",
    "plt.title(\"Log-10 of Mean KW | Last \" + str(last) + \" Trials \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the last \n",
    "plt.figure(figsize = (12, 8))\n",
    "\n",
    "for iS in range(NUM_SESSIONS):\n",
    "    plt.plot(cost_endT[last:-1, iS] - re_cost[last:-1, iS])\n",
    "\n",
    "plt.title(\"Difference Between Potential Function and Reach Error | Last \" + str(last) + \" Trials \")\n",
    "plt.xlabel(\"Trials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Cost Per Session:\")\n",
    "\n",
    "for iS in range(NUM_SESSIONS):\n",
    "    print(\"Session # \" + str(iS) + \"| Cost (start) = \" + str(np.mean(cost_startT[:last, iS])))\n",
    "    print(\"Session # \" + str(iS) + \"| Cost (end) = \" + str(np.mean(cost_startT[NUM_TRIALS-last-1:NUM_TRIALS-1, iS])))\n",
    "    print(\"\")\n",
    "    \n",
    "print(\"\")\n",
    "print(\"Average Reach Error Per Session:\")\n",
    "\n",
    "for iS in range(NUM_SESSIONS):\n",
    "    print(\"Session # \" + str(iS) + \"| Reach Error (start) = \" + str(np.mean(re_cost[:last, iS])))\n",
    "    print(\"Session # \" + str(iS) + \"| Reach Error (end) = \" + str(np.mean(re_cost[NUM_TRIALS-last-1:NUM_TRIALS-1, iS])))\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of 1st dimension, neuron #1 and 1st session \n",
    "\n",
    "for iS in range(NUM_SESSIONS):\n",
    "    plt.plot(k_trial[0, 0,:, iS]) #, label=''+sr(iS))\n",
    "plt.title(\"First Element of K matrix across trials\")\n",
    "plt.xlabel(\"Trials\")\n",
    "plt.ylabel(\"First Element\")\n",
    "plt.show()\n",
    "\n",
    "for iS in range(NUM_SESSIONS):\n",
    "    plt.plot(a_trial[0,:, iS]) #, label=''+sr(iS))\n",
    "plt.title(\"First Element of A matrix across trials\")\n",
    "plt.xlabel(\"Trials\")\n",
    "plt.ylabel(\"First Element\")\n",
    "plt.show()\n",
    "\n",
    "for iS in range(NUM_SESSIONS):\n",
    "    plt.plot(lambda_trial[0, 1, :, iS]) #, label=''+str(iS))\n",
    "plt.title(\"First Element of W matrix across trials\")\n",
    "plt.xlabel(\"Trials\")\n",
    "plt.ylabel(\"First Element\")\n",
    "plt.show()\n",
    "\n",
    "for iS in range(NUM_SESSIONS):\n",
    "    plt.plot(lambda_trial[0, 0,:, iS]) #, label=''+sr(iS))\n",
    "plt.title(\"First Element of Baseline across trials\")\n",
    "plt.xlabel(\"Trials\")\n",
    "plt.ylabel(\"First Element\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iS in range(NUM_SESSIONS):\n",
    "    plt.plot(k_trial[0, 0, NUM_TRIALS-last-1: NUM_TRIALS-1, iS]) #, label=''+sr(iS))\n",
    "plt.title(\"First Element of K matrix across trials | Last \" + str(last) + \" Trials \")\n",
    "plt.xlabel(\"Trials\")\n",
    "plt.ylabel(\"First Element\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean of Last K = \"+ str(np.mean(k_trial[0, 0, NUM_TRIALS-last-1: NUM_TRIALS-1,:])))\n",
    "\n",
    "for iS in range(NUM_SESSIONS):\n",
    "    plt.plot(lambda_trial[0, 1, NUM_TRIALS-last-1: NUM_TRIALS-1, iS]) #, label=''+str(iS))\n",
    "plt.title(\"First Element of W matrix across trials | Last \" + str(last) + \" Trials \")\n",
    "plt.xlabel(\"Trials\")\n",
    "plt.ylabel(\"First Element\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean of Last W = \"+ str(np.mean(lambda_trial[0, 1, NUM_TRIALS-last-1: NUM_TRIALS-1, :])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(k_trial))\n",
    "print(np.shape(lambda_trial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_mean = np.zeros([2, NUM_NEURONS])\n",
    "for iS in range(NUM_SESSIONS):\n",
    "    for iN in range(NUM_NEURONS):\n",
    "        start_mean[0, iN] = np.mean(k_trial[0, iN, 0: last,:])\n",
    "        print(\"Mean of Starting K [\" + str(iN) + \"] = \"+ str(conv_mean[0, iN]))\n",
    "        plt.plot(k_trial[0, iN, NUM_TRIALS-last-1: NUM_TRIALS-1, iS]) #, label=''+sr(iS))\n",
    "plt.title(\"K matrix across trials | Last \" + str(last) + \" Trials \")\n",
    "plt.xlabel(\"Trials\")\n",
    "plt.ylabel(\"K value\")\n",
    "plt.show()\n",
    "\n",
    "for iS in range(NUM_SESSIONS):\n",
    "    for iN in range(NUM_NEURONS):\n",
    "        start_mean[1, iN] = np.mean(lambda_trial[iN, 1, 0:last, :])\n",
    "        print(\"Mean of Starting W [\" + str(iN) + \"] = \"+ str(conv_mean[1, iN]))\n",
    "        plt.plot(lambda_trial[iN, 1, NUM_TRIALS-last-1: NUM_TRIALS-1, iS]) #, label=''+str(iS))\n",
    "plt.title(\"First Element of W matrix across trials | Last \" + str(last) + \" Trials \")\n",
    "plt.xlabel(\"Trials\")\n",
    "plt.ylabel(\"First Element\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_mean\n",
    "print(\"K.T*K = \" + str(np.inner(start_mean[0, :], start_mean[0, :])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_mean = np.zeros([2, NUM_NEURONS])\n",
    "for iS in range(NUM_SESSIONS):\n",
    "    for iN in range(NUM_NEURONS):\n",
    "        conv_mean[0, iN] = np.mean(k_trial[0, iN, NUM_TRIALS-last-1: NUM_TRIALS-1,:])\n",
    "        print(\"Mean of Last K [\" + str(iN) + \"] = \"+ str(conv_mean[0, iN]))\n",
    "        plt.plot(k_trial[0, iN, NUM_TRIALS-last-1: NUM_TRIALS-1, iS]) #, label=''+sr(iS))\n",
    "plt.title(\"K matrix across trials | Last \" + str(last) + \" Trials \")\n",
    "plt.xlabel(\"Trials\")\n",
    "plt.ylabel(\"K value\")\n",
    "plt.show()\n",
    "\n",
    "for iS in range(NUM_SESSIONS):\n",
    "    for iN in range(NUM_NEURONS):\n",
    "        conv_mean[1, iN] = np.mean(lambda_trial[iN, 1, NUM_TRIALS-last-1: NUM_TRIALS-1, :])\n",
    "        print(\"Mean of Last W [\" + str(iN) + \"] = \"+ str(conv_mean[1, iN]))\n",
    "        plt.plot(lambda_trial[iN, 1, NUM_TRIALS-last-1: NUM_TRIALS-1, iS]) #, label=''+str(iS))\n",
    "plt.title(\"First Element of W matrix across trials | Last \" + str(last) + \" Trials \")\n",
    "plt.xlabel(\"Trials\")\n",
    "plt.ylabel(\"First Element\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_mean[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start: \" + str(np.inner(start_mean[0, :], start_mean[0, :])) )\n",
    "print(\"end: \" + str(np.inner(conv_mean[0, :], conv_mean[0, :])) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start: \" + str(np.linalg.norm(start_mean[0, :], 2) ))\n",
    "print(\"end: \" + str(np.linalg.norm(conv_mean[0, :], 2)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_mean[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start: \" + str(np.inner(start_mean[1, :], start_mean[1, :])) )\n",
    "print(\"end: \" + str(np.inner(conv_mean[1, :], conv_mean[1, :])) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start: \" + str(np.linalg.norm(start_mean[1, :], 2) ))\n",
    "print(\"end: \" + str(np.linalg.norm(conv_mean[1, :], 2)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing norms of Ko and Wo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"K/W = \" + str(np.linalg.norm(conv_mean[0, :], 2)/np.linalg.norm(conv_mean[1, :], 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KW Convergence Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"inner product of K and W\")\n",
    "print(np.inner(conv_mean[0, :], conv_mean[1, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"inner product / KW\")\n",
    "print(np.inner(conv_mean[0, :], conv_mean[1, :])/(np.linalg.norm(conv_mean[0, :], 2)*np.linalg.norm(conv_mean[1, :], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "for iN in range(NUM_NEURONS):\n",
    "    plt.plot(fr_trial[iN, 0, :-1], label='' + str(iN))\n",
    "plt.legend()\n",
    "plt.xlabel('Trials')\n",
    "plt.ylabel('Firing Rate (Hz)')\n",
    "plt.title('Firing Rate Across Trials')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECODER_ADAPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BRAIN_ADAPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationary Point Analysis\n",
    "\n",
    "$$ k_0 = \\sqrt{-\\frac{\\sigma}{\\tau^2} + 1} $$\n",
    "$$ w_0 = \\sqrt{-\\frac{\\sigma}{\\tau^2} + 1} $$\n",
    "\n",
    "$$ k_0w_0 = 1 -\\frac{\\sigma}{\\tau^2} $$\n",
    "\n",
    "With $\\sigma = \\frac{1}{2}$ and $\\tau = 1$\n",
    "$$ k_0w_0 = \\frac{1}{2} $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_dist = SIGMA_GAUSSIAN\n",
    "mu = 0\n",
    "\n",
    "s = np.random.normal(mu, sigma_dist, 1000)\n",
    "count, bins, ignored = plt.hist(s, 30, density=True)\n",
    "plt.plot(bins, 1/(sigma_dist * np.sqrt(2 * np.pi)) *\n",
    "               np.exp( - (bins - mu)**2 / (2 * sigma_dist**2) ),\n",
    "         linewidth=2, color='r')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
